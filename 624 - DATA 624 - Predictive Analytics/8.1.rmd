---
title: '8.1'
author: "Joseph Elikishvili"
date: "December 12, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

## 8.1

We will load data from 7.2 as per instructions.

```{r}
library(mlbench)
set.seed(202)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

### Fit a random forest model to all of the predictors, then estimate the variable importance scores:

```{r}
library(randomForest)
library(caret)
model1 <- randomForest(y ~ ., data = simulated,
importance = TRUE,
ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1
```

We can see that V1-V5 got much higher weights then V6-V10. By rough estimate we see a cumulative absolute weight of 25 for V1-V5 and an absolute cumulative weight of about 0.6 for V6-V10, this shows a significant relative difference
Did the random forest model significantly use the uninformative predictors (V6 - V10)?



### B. Now add an additional predictor that is highly correlated with one of the informative predictors. For example:

```{r}
duplicate1 <- simulated$V1 + rnorm(200) * .1
#cor(simulated$duplicate1, simulated$V1)
```

Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?

Lets go ahead and add the predictor to the data and re run the model
```{r}
simulated$V11 = duplicate1
model2 <- randomForest(y ~ ., data = simulated,
importance = TRUE,
ntree = 1000)
rfImp2 <- varImp(model2, scale = FALSE)
rfImp2
```

We can see that V1 score has decreased, as the highly corelated predictor has been added. So we can conclude that the importance of the variable will decrase if there are highly corelated predictors present.

### C. Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?

Lets run the model and review results
```{r}
library(party)
rf1 = cforest(y ~., data =simulated[, 1:11], controls = cforest_control(ntree=1000))
rf2 = cforest(y ~., data =simulated, controls = cforest_control(ntree=1000))
imp1= varImp(rf1) 
imp2= varImp(rf2)
imp1
```

```{r}
imp2
```
 
We can see a smiliar behaivior, V1-V5 have the higherst importance while V6-V10 have the lower importance.


### D. Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?

Lets try to generate results with Bagged Trees

```{r}
library(ipred)
bt1 = bagging(y ~., data = simulated[, 1:11], nbag =50)
bt2 = bagging(y ~., data = simulated, nbag =50)
imp_bt1= varImp(bt1) 
imp_bt2= varImp(bt2)
imp_bt1
```

```{r}
imp_bt2
```

Bagged Trees are changing some of the results, we can see that V10 has a higher then normal importance and we see a more distributed importance then before, V6-V10 are no long just a franction of the V1-V5 importance levels.

Next we will try Cubist model

```{r}
library(Cubist)
cb1 = cubist(simulated[, 1:10], simulated$y, committees =100)
cb2 = cubist(simulated[, names(simulated) !="y"], y=simulated$y, committees =100 )
imp_cb1 = varImp(cb1)
imp_cb2 = varImp(cb2)
imp_cb1
```

```{r}
imp_cb2
```

Cubist model is similar to the first models V1-V5 have a much higher importance then V6-V10, although we see that V7 has moved up relative to V6-V10.


#### 8.3. In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:

#### (a) Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?

The model on the right focuses its importance on just first few predictors becuase of the fact that as the learning rate increases, it becomes more greedy and will use fewer predictors. Also another reason is due to bagging fraction, the higher is the fraction, less predictors will be identified as important.


#### (b) Which model do you think would be more predictive of other samples?

The left model is using more predictors, while the model on teh right relies mostly on top 3 predictors, so assumption can be made the the model on the left will be more accurate.

#### (c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?

Increasing Interaction depth will most likely force the model to spread the impportance across predictors, so the model on the right will probably benefit more from it, but as a result, more weight wil be icnreased to predictors lagging importance.

#### 8.7  Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

We will import imputation and data splitting code from 6.3 below.
```{r}
library(AppliedPredictiveModeling)
library(caret)

data(ChemicalManufacturingProcess)
df = ChemicalManufacturingProcess
#summary(df)

library(missForest)
df_imp1 = missForest(df)
df_imp = df_imp1$ximp

data = df_imp[,2:58]
target = df_imp[,1]

training = createDataPartition( target, p=0.75 )
predictor_training = data[training$Resample1,]
target_training = target[training$Resample]

predictor_testing = data[-training$Resample1,]
target_testing = target[-training$Resample1]
ctrl <- trainControl(method = "cv", number = 10)
```

We will use several models and will evaluate the results.

#### Regression tree

First we will use Single Tree model and evaluate the results.
```{r}

#Tune
rt_grid <- expand.grid(maxdepth= seq(1,10,by=1))
rt_Tune <- train(x = predictor_training, y = target_training, method = "rpart2", metric = "Rsquared", tuneGrid = rt_grid, trControl = ctrl)

#Predict
rt_pred = predict(rt_Tune, predictor_testing)
postResample(pred = rt_pred, obs = target_testing)
```

Next we will use Random forest model to evaluate 

```{r}
#Tune
rf_grid <- expand.grid(mtry=seq(2,38,by=3))
rf_Tune <- train(x = predictor_training, y = target_training, method = "rf", tuneGrid = rf_grid, metric = "Rsquared", importance = TRUE, trControl = ctrl)

#Predict
rf_pred = predict(rt_Tune, predictor_testing)
postResample(pred = rt_pred, obs = target_testing)
```

Finally lets take a look at Cubist model

```{r}

#Tune
cube_grid <- expand.grid(committees = c(1, 5, 10, 20, 50), neighbors = c(0, 1, 3, 5))

cube_Tune <- train(x = predictor_training, y = target_training, method = "cubist", metric = "Rsquared", tuneGrid = cube_grid, trControl = ctrl)

#Predict
cube_pred = predict(cube_Tune, predictor_testing)
postResample(pred = cube_pred, obs = target_testing)
```

It appear that Cubist model has the best RMSE score, it is an improvement over MARS scores and over PLS scores that we had.

#### (b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

Lets go ahead and review the predictor importance of cubist model

```{r}
plot(varImp(cube_Tune), top=10, scales = list(y = list(cex = 0.8)))
```

We can see that Manufacturing process32 made the top of the list which is not a surprise as we already established that it is the most importnant predictor, followed by Manufacturingprocess17. One thing to notice is that Cubist model heavily relys on top 2 predictors vs PLS, but we saw similar behaivior with MARS. One suprice is that Cubist is using Manufacturer process 17 and not 9 like MARS, but then agaon we saw similar usage with PLS and other models. 

#### C. Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

Next we will plot the OPtimal Sinlge tree

```{r}
library(party)
library(partykit)
plot(as.party(rt_Tune$finalModel),gp=gpar(fontsize=11))
```

As expected we can see the the top predictors are Manufacture related processes. Manufacturing process32 is at the top only few Biological processes affect target in any meaningful way. 

#### Conlcusion.

At this point we have established that manufacturer processes have a much larger impacting role towards yild then biaological factors, Cubist model is the latest and strongest confirmation of that, so any further study should be focused on finding optimal values for manufacturing proceses.


